# WAN 2.1 LoRA — Human Activity Recognition Video Synthesis
(Autogenerated skeleton — 2025-10-25 20:13:03.677658)

This document summarizes setup, training, and qualitative analysis for generating 10 exemplar videos per action class using the Kaggle *Human Activity Recognition (Video)* dataset.

## TL;DR
- **Model**: WAN 2.1 *1.3G* backbone with LoRA adapters (rank 64 by default)
- **Dataset**: Kaggle Human Activity Recognition (video); categories used: `{to_fill}`
- **Training budget**: {to_fill} GPU hours on RunPod (A100/24-80GB recommended)
- **Sampling**: 40–60 steps, DDIM / DPM-Solver (cfg 2.0–4.0), 512x512 @ 16–24 fps
- **Deliverables**: 10 videos per class; grid sheet + gallery

## What we analyzed
- **Prompting schema** per class
- **Temporal coherence** (scene consistency, motion smoothness)
- **Identity/object persistence**
- **Action recognizability** (does a human label the class correctly?)
- **Artifact frequency** (flicker, limb melt, texture crawl)
- **Diversity** across 10 samples (backgrounds, viewpoints, actors)

## Key Results
> Fill in once renders are produced. Insert 2–3 representative GIFs or MP4 thumbnails per class and a 10-sample contact sheet.

## Pass/Fail Heuristics (for grading)
| Criterion | Heuristic |
| --- | --- |
| Recognizability | ≥ 8/10 samples per class labeled correctly by a human |
| Coherence | ≥ 80% frames pass optical‑flow smoothness threshold |
| Diversity | ≥ 4 distinct camera angles/contexts across 10 samples |

## Reproduction on RunPod
1. `pip install -r requirements.txt`
2. `python scripts/prepare_kaggle.py --dataset sharjeelmazhar/human-activity-recognition-video-dataset --out data/har`
3. `python train_lora.py --config configs/lora_wan21_har.yaml`
4. `python generate_videos.py --config configs/infer_har.yaml`

## Notes
- WAN 2.1 1.3G selected to meet VRAM constraints; avoid 14G due to pod limits.
- LoRA keeps base frozen → stable training on limited budget, faster iteration.
- Use **EMA weights** for sampling for smoother motion.
