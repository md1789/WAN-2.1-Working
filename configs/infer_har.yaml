model:
  name: wan-2.1-1.3b
  base_ckpt: "Wan-AI/Wan2.1-T2V-1.3B-Diffusers"
  lora_path: "outputs/lora_har/lora_ema_last"
  framepack_ckpt: "tencent/HunyuanVideo-Framepack"         # your actual FramePack pipeline ID
  framepack_transformer: "tencent/HunyuanVideo-Framepack-Transformer"
  framepack_flux: "google/siglip-so400m-patch14-384"       # must match the exact encoder used when training
  framepack_t2i: "black-forest-labs/FLUX.1-dev"   

sampler:
  frames: 32             # increase for longer videos
  fps: 8
  size: 448
  steps: 20
  cfg_scale: 6.0
  seed: 42
  vae_t_chunk: 3         # safe default; lower for limited VRAM
  negative_prompt: ""    # optional

dataset:
  classes: ["walking", "running", "jumping", "waving"]
  per_class: 1
  out_dir: "outputs/samples"
